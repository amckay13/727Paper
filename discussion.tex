\section{Discussion}
\label{sec:discussion}

\subsection{Future Work}
\label{sec:futurework}
There are many ways this would could be extended in the future. For
example, in the original paper, the researchers also explore E3 and E4
terms that pushes hidden unit activation vectors apart if they are from different
output classes and pushes them closer together if they are from the
same output class. That theory could be extended to multi-layer hidden
networks as well. Additionally, the researchers implemented pruning,
which removed weights deemed unnecessary between the input and hidden
layer, while we did not. Experiments could be run that implement
pruning between all, or just between some layers in our more
complicated structure. During the clustering phase of rule extraction, it
is interesting to note that in practice many of the intervals generated are very close
to one another. One could prune rules to be simpler by merging
intervals that are within some threshold of one another regardless of
output class, which would lead to a more interpretable set of rules in exchange for decreased
rule accuracy. We leave any experimentation with such a threshold for
future work. We also limited our network structure to fully
connected feedforward networks, while in practice many networks that
have multiple hidden layers are recurrent or convolutional. Optimizing
rule extraction for those more complicated structures could also be an
avenue for future work. 

\subsection{Limitations}
\label{sec:limitations}
There are many limitations to our work. There are many
hyper parameters to consider in the neural networks, including the
$\beta$ values, the activation function, the weight decay $\lambda$,
our mechanism for early stopping, and the number of epochs
\todo{missing any}. It is possible that other configurations of these
parameters could lead to fewer rules generated and/or better rule
accuracy. We also did not have access to a full suite of GPUs
and ran our experiments on local desktops, which meant runs took
hours, sometimes even days. This constraint on our resources affected
how many iterations of the experiment we could do. It also limited us to networks with at most 3 hidden layers, which may not be enough
to see if the approach would work at scale. Additionally, we
used the C5.0 suite to perform rule extraction, but this is the newest
release of the suite and is not well-documented like prior releases. It is
therefore possible there are optimizations for rule extraction that we
did not know to include. The rules yielded by rule extraction, as the representative example above shows, often appear to overfit by containing many disjuncts that are singleton intervals, reducing conditions to strict equality of the activations against single values, which would not generalize well for test data. Examining the spread of a node's activations prior to clustering reveals overlap for various regions, which can lower accuracy as such a region cannot be used in rules to discriminate.

\subsection{Conclusion}
\label{sec:conclusion}

% From Reggia: What were findings? Visualize results
