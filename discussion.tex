\section{Discussion}
\label{sec:discussion}

\subsection{Future Work}
\label{sec:futurework}
There are many ways this would could be extended in the future. For
example, in the original paper, the researchers also explore E3 and E4
terms that pushes hidden unit activation vectors apart if they are from different
output classes and pushes them closer together if they are from the
same output class. That theory could be extended to multi-layer hidden
networks as well. Additionally, the researchers implemented pruning,
which removed weights deemed unnecessary between the input and hidden
layer, while we did not. Experiments could be run that implement
pruning between all, or just between some layers in our more
complicated structure. During the clustering phase of rule extraction, it
is interesting to note that in practice many of the intervals generated are very close
to one another. One could prune rules to be simpler by merging
intervals that are within some threshold of one another regardless of
output class, which would lead to a more interpretable set of rules in exchange for decreased
rule accuracy. We leave any experimentation with such a threshold for
future work. We also limited our network structure to fully
connected feedforward networks, while in practice many networks that
have multiple hidden layers are recurrent or convolutional. Optimizing
rule extraction for those more complicated structures could also be an
avenue for future work. 

\subsection{Limitations}
\label{sec:limitations}
There are many limitations to our work. For instance, there are many
hyper parameters to consider in the neural networks, including the
$\beta$ values, the activation function, the weight decay $\lambda$,
our mechanism for early stopping, and the number of epochs
\todo{missing any}. We attempted to optimize these for maximal network
performance, but it is possible there are configurations that would
perform better. We also did not have access to a full suite of GPUs
and ran our experiments on local desktops, which meant runs took
hours, sometimes even days. This constraint on our resources affected
how many iterations of the experiment we could do. Additionally, we
used the C5.0 suite to perform rule extraction, but this is the newest
release of the suite and does not come with documentation. It is
therefore possible there are optimizations for rule extraction that we
did not know to include.

\subsection{Conclusion}
\label{sec:conclusion}

% From Reggia: What were findings? Visualize results
