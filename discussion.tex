\section{Discussion}
\label{sec:discussion}

\subsection{Future Work}
\label{sec:futurework}
There are many ways this would could be extended in the future. For
example, in the original paper, the researchers also explore E3 and E4
terms that pushes hidden unit activation vectors apart if they are from different
output classes and pushes them closer together if they are from the
same output class. That theory could be extended to multi-layer hidden
networks as well. We also limited our network structure to fully
connected feedforward networks, while in practice many networks that
have multiple hidden layers are recurrent or convolutional. Optimizing
rule extraction for those more complicated structures could also be an
avenue for future work. 

\subsection{Limitations}
\label{sec:limitations}
There are many limitations to our work. For instance, there are many
hyper parameters to consider in the neural networks, including the
$\beta$ values, the activation function, the weight decay $\lambda$,
our mechanism for early stopping, and the number of epochs
\todo{missing any}. We attempted to optimize these for maximal network
performance, but it is possible there are configurations that would
perform better. We also did not have access to a full suite of GPUs
and ran our experiments on local desktops, which meant runs took
hours, sometimes even days. This constraint on our resources affected
how many iterations of the experiment we could do. Additionally, we
used the C5.0 suite to perform rule extraction, but this is the newest
release of the suite and does not come with documentation. It is
therefore possible there are optimizations for rule extraction that we
did not know to include.

\subsection{Conclusion}
\label{sec:conclusion}

% From Reggia: What were findings? Visualize results
