\begin{abstract}
 Many papers have explored different methods to extract and represent the interpretability of neural networks, especially as neural networks become more prevalent as tools used to address the worldâ€™s hardest problems. One approach inside explainable AI is broadly defined as rule extraction, the endeavor to create human-readable semantic rules that define the decision-making process of the network. Huynh et al \cite{thuan11} approached the rule extraction in a different way than most; instead of increasing the complexity of the rule extraction algorithm, they instead modified back propagation to encourage the network to learn hidden activation vectors that are farther apart. Because they successfully decrease the number of rules generated without sacrificing rule accuracy, we look to extend this work to more complex neural networks with multiple hidden layers. We hypothesized that this would be less successful in deeper networks, and so we perform similar experiments on five datasets with neural networks of varying depths (one, two, and three hidden layers). We conclude that while we are able to decrease the number of rule intervals generated without decreasing rule accuracy in some cases, in general this modification to backpropagation does not generalize to deeper neural networks. 
\end{abstract}

