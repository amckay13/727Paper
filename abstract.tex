\begin{abstract}
  Many papers have explored different methods to extract and represent
  the interpretability of neural networks, especially as neural networks
  become more prevalent. One approach to this is rule extraction,
  the endeavor to create human-readable semantic rules that define
  the decision-making process of the network. Huynh et al \cite{thuan11}
  approached rule extraction in a different way than most; instead of
  increasing the complexity of the rule extraction algorithm, they instead
  modified error backpropagation to encourage the network to learn hidden activation
  vectors that are farther apart. This approach successfully decreased the
  number of rules generated without sacrificing rule accuracy in
  single layer hidden networks. We look to
  extend this work to more complex neural networks with multiple hidden layers.
  We hypothesized that this would be less successful in deeper networks,
  and so we perform similar experiments on six datasets with neural networks
  of varying depths (one, two, and three hidden layers). We conclude that
  while we are able to decrease the number of rule intervals generated
  without decreasing rule accuracy in some cases, in general this
  modification to backpropagation does not generalize to deeper neural networks. 
\end{abstract}

