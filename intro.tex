\section{Introduction}

% from Reggia: What's the problem? hypothesis?
\label{sec:intro}

Neural networks are powerful computational tools used across a diverse set of industries, but as the configurations of these networks evolve, the interpretability of the outcomes often becomes more opaque. This presents a problem as networks are increasingly used to make important decisions across many fields including medicine, finances, and military. In order to be trusted to make these important decisions, it is imperative to achieve \textit{Explainaible Artificial Intelligence} (XAI), meaning that humans can interpret how neural networks function.

XAI is a relatively new field of research, however there is some significant prior work on which we build. Many previous approaches to XAI have used visual aids to interpret network behaviour. Bau et al propose a new visual framework that quantifies the interpretability of Convolutional Neural Networks (CNNs) by analyzing their hidden units’ ties to semantic meaning \cite{bau2017network}. Olah et al use feature visualization and attribution techniques to achieve a different representation of what the network is learning at the hidden layers \cite{ olah2018the}. Ramanishka et al. produce saliency heat maps that better showcase the image-region to generated word relationship in encoder-decoder networks \cite{ramanishka2017top}. Lent et al create interactive training explanation systems tool which users can use to explore network behavoiur \cite{van2004explainable}.

One particularly promising approach to interpretting neural networks is decompositional rule extraction, a systematic approach for describing the approximate behaviour of networks in an understandable and accurate way. For example, Shrikumar et al create DeepLIFT, a system which decomposes output by analyzing the activations of a node and scoring the neurons that contribute to it \cite{shrikumar2017learning}. Older work on rule extraction \cite{duch2001new, tsukimoto2000extracting} uses decision trees \cite{ baehrens2010explain} to describe hidden node activations. Martens et al use an
active-learning approach to create comprehensible rules for Support Vector Machines (SVMs) \cite{martens2009decompositional}.

While much of the previous work has focused on methods for extracting rules from
networks ex post facto, a few approaches have focused on adjustments to the network
during training to improve interpretability. Yang et al
train networks to produce similar and dissimilar examples to help
'explain' its decisions to humans \cite{yang2017explainable}.
Huynh et al propose
an additional error term that encourages hidden unit activation vectors
to separate by class causing clustering in more compact groups
which leads to improved rule extraction \cite{thuan11}.

Huynh et al's work is particularly promising as it shows significant reductions in
rules generated for networks trained on a number of complex datasets.
However, they only test their approach on networks with a single hidden
layer. While technically no more than a single hidden layer is needed, deep
network architectures have become a popular tool for difficult machine learning
tasks. To date, no work has extended
the work of Huynh et al to deeper feedforward networks.
Our work intends to apply a new error term for hidden node separation
in multilayer neural networks. We hypothesize that including the new error
function will not be able to yield both a significant reduction in rules
and a comparable accuracy of those rules for rule extraction, as multilayer
networks introduce significant complexity to the problem that Huynh et al
did not encounter in single-hidden-layer networks.
We test our hypothesis by training multi-layer neural networks
with and without the added error term and comparing number and accuracy
of rules generated via rule extraction.

%% This is not a new issue, there has been prior work that endeavors to capture the neural network’s process by training explanation systems a user can interact with \cite{van2004explainable} or training the network to produce similar and dissimilar examples to ‘explain’ its decisions  \cite{yang2017explainable}. Aptly, the focus of explainability has shifted somewhat to a popular problem, image captioning and/or recognition. Ramanishka et al. produced saliency heat maps that better showcase the image-region to generated word relationship in encoder-decoder networks \cite{ramanishka2017top}. Additionally, Bau et al proposed a new framework that quantifies the interpretability of CNN’s by analyzing their hidden units’ ties to semantic meaning \cite{bau2017network}, while Olah et al used feature visualization and attribution techniques to achieve the same goal, a representation of what the network is learning at the hidden layers \cite{ olah2018the}.
%% In addition to the approaches enumerated above, there exists a section of work that endeavors to re-construct and model the logical rules/systems that the network develops during training. For example, Shrikumar et al. create DeepLIFT, a system which decomposes output by analyzing the activations of a node and scoring the neurons that contribute to it \cite{shrikumar2017learning}. There is also older work that attempts to create decision trees \cite{ baehrens2010explain}, but both of these approaches fall under the general idea of decompositional rule extraction, the idea of systematically defining, in a human parse-able way, what the network has learned based on analyzing the hidden nodes \cite{duch2001new} \cite{tsukimoto2000extracting}. This most often involves tracking activations to create rules between the output and hidden layers, and between the hidden and input layers, then combing those rules to create ‘network’ rules. In a key insight, Thuan et al. realized that because the main constraint on the neural network is to minimize error at the output, hidden layers can encode quite convoluted patterns, which in turn means rule extraction is difficult and produces a large number of complicated conditions to accurately capture that fact \cite{thuan11}. They proposed, and implemented, an additional error term that encourages the hidden unit activation vectors to separate themselves farther by class, clustering in more compact groups with the intent to make rule extraction generate fewer rules. These modifications successfully lowered the number of rules generated without performing worse in terms of classification accuracy, however their work was limited to networks with a single hidden layer. 
%% Our goal is to extend the work one by Thuan et al. and analyze adding this new error term to deeper neural networks, with multiple hidden layers. We hypothesize that including the new error function from the paper will not necessarily lead to a significant reduction in rules due to the added complexity of multiple layers. This work is pertinent because there are many datasets and problems that, in practice, require more complicated, deep network structure. The complicated structure of deeper networks makes them a good target for this procedure of rule extraction, as they are much harder for humans to understand than a one-hidden-layer network.

