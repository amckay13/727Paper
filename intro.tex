\section{Introduction}

% from Reggia: What's the problem? hypothesis?
\label{sec:intro}

Neural Networks are powerful computational tools used across a diverse set of industries, but as the configurations of these networks evolve and become even more powerful, the interpretability of the outcomes simultaneously becomes more opaque. This presents a problem when networks are used to make important medical, financial, linguistic, and military decisions; people can be reticent to trust machines with choices that can affect that much change when the rationale behind the decision is obscured in a black box. This is not a new issue, there has been prior work that endeavors to capture the neural network’s process by training explanation systems a user can interact with \cite{van2004explainable} or training the network to produce similar and dissimilar examples to ‘explain’ its decisions  \cite{yang2017explainable}. Aptly, the focus of explainability has shifted somewhat to a popular problem, image captioning and/or recognition. Ramanishka et al. produced saliency heat maps that better showcase the image-region to generated word relationship in encoder-decoder networks \cite{ramanishka2017top}. Additionally, Bau et al proposed a new framework that quantifies the interpretability of CNN’s by analyzing their hidden units’ ties to semantic meaning cite{bau2017network}, while Olah et al used feature visualization and attribution techniques to achieve the same goal, a representation of what the network is learning at the hidden layers \cite{ olah2018the}.
In addition to the approaches enumerated above, there exists a section of work that endeavors to re-construct and model the logical rules/systems that the network develops during training. For example, Shrikumar et al. create DeepLIFT, a system which decomposes output by analyzing the activations of a node and scoring the neurons that contribute to it \cite{shrikumar2017learning}. There is also older work that attempts to create decision trees \cite{ baehrens2010explain}, but both of these approaches fall under the general idea of decompositional rule extraction, the idea of systematically defining, in a human parse-able way, what the network has learned based on analyzing the hidden nodes \cite{duch2001new} \cite{tsukimoto2000extracting}. This most often involves tracking activations to create rules between the output and hidden layers, and between the hidden and input layers, then combing those rules to create ‘network’ rules. In a key insight, Thuan et al. realized that because the main constraint on the neural network is to minimize error at the output, hidden layers can encode quite convoluted patterns, which in turn means rule extraction is difficult and produces a large number of complicated conditions to accurately capture that fact \cite{thuan11}. They proposed, and implemented, an additional error term that encourages the hidden unit activation vectors to separate themselves farther by class, clustering in more compact groups with the intent to make rule extraction generate fewer rules. These modifications successfully lowered the number of rules generated without performing worse in terms of classification accuracy, however their work was limited to networks with a single hidden layer. 
Our goal is to extend the work one by Thuan et al. and analyze adding this new error term to deeper neural networks, with multiple hidden layers. We hypothesize that including the new error function from the paper will not necessarily lead to a significant reduction in rules due to the added complexity of multiple layers. This work is pertinent because there are many datasets and problems that, in practice, require more complicated, deep network structure. The complicated structure of deeper networks makes them a good target for this procedure of rule extraction, as they are much harder for humans to understand than a one-hidden-layer network.

