\section{Results}
\label{sec:results}

To test the effectiveness of our new error term for rule extraction,
we applied our approach to five \todo{will we actually do 5 datasets}
datasets from the UCI Machine Learning Repository \cite{uci}. Our
datasets are all classifications problems, covering a wide-range of
use-cases. \textit{Iris} is a classic classification dataset which
contains iris flower types and four associated length
measurements of the petal and sepal of the flower. \textit{Ecoli}
records the localization site of a protein and a number of
physical measurements including amino acid content of the outer membrane
and periplasmic proteins. \textit{Ionosphere} contains radar data collected
from high-frequency antennas and whether or not the collected data
contains evidence of the ionosphere. \textit{Seeds} describes three
varieties of wheat and geometrical properties of their kernels.
Finally, \textit{Waveform} describes three different types of waves
using 40 noisy measurements, where noise is generated normally
(with a mean of 0 and variance of 1).
An overview of our datasets is given in \ref{tab:datasets}.

We ran each dataset using two different approaches. The first
used only standard mean-squared error $E_1$
\footnote{Note that we use $E_1$ to refer to $E_1+E_3$ for simplicity, as
$E_3$ is included for both approaches}. The second used both
$E_1$ and our node separation term $E_2$. For each run, we recorded
the accuracy on the validation data and the $E_2$ error.
We first test to see if including $E_2$ in our error causes either
a significant decrease in accuracy or a significant increase
in $E_2$ hidden node separation. We then use C5.0 rule extraction
to assess if including $E_2$ leads to fewer rules generated.

\begin{table}[]
  \centering
\begin{tabular}{@{}llll@{}}
\toprule
Dataset             & Attributes & Classes & Samples \\ \midrule
\textit{Iris}       & 4          & 3       & 150     \\
\textit{Ecoli}      & 7          & 8       & 336     \\
\textit{Ionosphere} & 34         & 2       & 351     \\
\textit{Seeds}      & 7          & 3       & 210     \\
\textit{Waveform}   & 21         & 3       & 5000    \\ \bottomrule
\end{tabular}
\caption{UCI Machine Learning Datasets}
\label{tab:datasets}
\end{table}

There were a number of important design decisions for our networks
which we describe briefly below:

\begin{enumerate}
\item
  We tested every dataset on three different networks, with one,
  two, and three hiden layers respectively. Each hidden layer had
  four nodes \footnote{We used four as this is what was used in \cite{thuan11}}.
\item
  All weights in the networks were initialized by sampling from the
  uniform distribution from $[-1,1]$. Error backpropagation used RPROP
  to update weights.
\item
  $\beta$ values are critical to network performance. Thus, we automated
  tests to find $\beta$ values for each dataset and network using the
  following algorithm
  \begin{enumerate}
  \item
    Start $\beta$ at some reasonable upper limit ($0.01$).
  \item
    Use $\beta$ value to train four new networks. Also train four
    networks using only standard $E_1$ error.
  \item
    Assess accuracy of networks on testing data. If average accuracy
    across the four networks is greater than $0.05$ off of the average
    accuracy of the $E_1$ nets, repeat the process with
    $\beta = \frac{\beta}{2}$. If not, return $\beta$.    
  \end{enumerate}
  $\alpha$ simply initialized to $1-\beta$ after this process.
\item
  We set the weight decay factor $\lambda$ to 0.00001 as used in \cite{thuan11}.
\item
  In practice, despite the weight decay factor, some weights become large which
  can cause hidden node activations to be very close to $1$ ($1$ with floating
  point cutoff) regardless of the input. These nodes do not help cluster inputs
  and add unnecessary complication to rule extraction. Thus, for any node which
  reports an activation of $1$ for all validation data is pruned from the
  network.
\item
  We implemented early stopping to make training quicker. We use a simple
  method which stops after $100$ epochs if a new best accuracy on the
  training data has not been achieved. In practice, we found $100$ epochs is
  a big enough window that it does not significantly degrade performance.
  We run for a maximum of $400$ epochs.
\item
  $20\%$ of the data is reserved for validation, while the other $80\%$ is
  used for training. The data is randomly partitioned for each dataset and
  each network, but is kept the same for comparison of just $E1$ and $E1$
  with $E2$ for the same network/dataset.
\item
  Each network encodes outputs using standard one-hot encoding, where the number
  of output nodes is the number of classes, and the predicted class is the
  output node with the maximum activation.
\item
  We ran each dataset on each network 21 times. We computed both the average
  mean-squared error $E_1/N$ as well as the average node separation
  $-E_2/N^2$.
\end{enumerate}

\begin{table*}[t!]
  \centering
  \small
  \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|}
    \hline
    Dataset & 
    \multicolumn{6}{c|}{Avg Val Acc} & 
    \multicolumn{6}{c|}{$-2E_2/N^2$} \\
    \cline{2-13}
    & \multicolumn{3}{c|}{$E_1$ only} &
    \multicolumn{3}{c|}{$E_1$ and $E_2$} &
    \multicolumn{3}{c|}{$E_1$ only} &
    \multicolumn{3}{c|}{$E_1$ and $E_2$} \\
    \cline{2-13}
    & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 \\
    \hline
    \textit{Iris} & \textbf{0.98} & \textbf{0.98} & 0.98 & 0.90 & 0.89 & 0.96 & 0.03 &0.08 & 0.10 & \textbf{0.07} & \textbf{0.10} & \textbf{0.13} \\
    \textit{Ecoli} & \textbf{0.85} & \textbf{0.83} & 0.79 & 0.73 & 0.78 & 0.79 & 0.01 & 0.05 & 0.07 & \textbf{0.05} & \textbf{0.09} & \textbf{0.10} \\
    \textit{Ionosphere} & \textbf{0.94} & \textbf{0.96} & 0.94 & 0.90 & 0.91 & 0.92 & 0.04 & 0.07 & 0.11 & \textbf{0.06} & \textbf{0.11} & \textbf{0.16} \\
    \textit{Seeds} & 0.88 & \textbf{0.89} & 0.86 & 0.85 & 0.85 & 0.84 & 0.01 & 0.03 & 0.06 & \textbf{0.04} & \textbf{0.06} & \textbf{0.09} \\
    \textit{Waveform} & \textbf{0.85} & \textbf{0.85} & \textbf{0.85} & 0.80 & 0.84 & 0.82 & 0.06 & 0.11 & 0.14 & \textbf{0.08} & \textbf{0.14} & \textbf{0.21} \\
    \hline
  \end{tabular}
  \caption{Average Validation Accuracy and Hidden Layer Node Separation for Datasets \todo{fill out table}}
  \label{tab:e1_e2_avgs}  
\end{table*}

%From Reggia: What were the findings?Visualizations for the results
