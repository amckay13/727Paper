\section{Methodology}
\label{sec:method}

\subsection{New Error Term}
\label{sec:nn}

In this paper, we are considering standard feed-forward networks with
sigmoid activation functions for all nodes in the network. More formally,
for all layers $l$ in the network, the activation of the $j^{th}$ node
$a_{H_{l_j}}^p$ for the $p^{th}$ input pattern is given by

\begin{equation}
  a_{H_{l_j}}^p = \sigma(\sum_{i\in H_{l-1}}{w_{ji}a_{H_{l-1_i}}^p})
\end{equation}

where $\sigma$ is the standard sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$
and $w_{ji}$ is the weight from node $i$ in the previous layer to node $j$.

We use standard mean-squared error term
\footnote{
It should be noted that using
mean-squared error is not standard for neural networks used for
classification, however we use mean-squared error as this is the error
used by \cite{thuan11} who use a similar approach for simplifying rule
extraction in single-layer feed-forward nets.
}

\begin{equation}
  E_1 = \frac{1}{2}\sum_{p=1}^{N} \sum_{k \in output} (t_k^p-a_{O_k}^p)^2
\end{equation}

where $N$ is the number of input patterns and $t_k^p$ is the target activation
of output node $k$ for input pattern $p$.

We introduce a new error term $E_2$ which is proportional to the distance between
activations in each hidden layer

\begin{equation}
  E_2 = -\frac{1}{2}\sum_{p=1}^{N} \sum_{q=1}^{N} \sum_{l=1}^{M} \sum_{k \in H_l} (a_{H_{l_k}}^p - a_{H_{l_k}}^q)^2
\end{equation}

where $M$ is the number of hidden layers.

We add a third error term $E_3$ that aims to minimize our weights, given
by

\begin{equation}
  E_3 = \sum_{j} \sum_{i} w_{ji}^2
\end{equation}

We define our total error term as

\begin{equation}
  E = \alpha E_1 + \beta E_2 + \lambda E_3
\end{equation}

where $0 \leq \beta \leq 1$ and $\alpha = 1 - \beta$.
We can view the problem as a multi-objective optimization:
minimizing $E_1$ will lead to better accuracy while
minimizing $E_2$ will create more separation between activations of
nodes in the hidden layers. The goal of the $\alpha$ and $\beta$
multipliers is to balance the error such that we achieve
both good accuracy and separation. The $E_3$ term is included
to minimize our weights; large weights will push our sigmoid
activations to $1$ which will in turn create less separation in
hidden node activations.

We calculate our learning rule based on $E$ via

\begin{equation}
  \frac{\partial E}{\partial w_{ji}} = \alpha \frac{\partial E_1}{\partial w_{ji}} + \beta \frac{\partial E_2}{\partial w_{ji}} + \lambda \frac{\partial E_3}{\partial w_{ji}}
\end{equation}

where $\frac{\partial E_1}{\partial w_{ji}}$ is the standard mean-squared error gradient

\begin{equation}
  \frac{\partial E_1}{\partial w_{ji}} = -\delta_{L_j}a_{L-1_i} 
\end{equation}

where $L$ is the layer that contains node $j$, $L-1$ is the previous layer
which contains node $i$, and $\delta_{L_j}$ is given by

\begin{equation}
  \delta_{L_j} =
  \begin{cases}
    (t_{O_j} - a_{O_j})a_{O_j}(1-a_{O-j}) & j \in L = O \\
    (\sum_{k \in L+1}{\delta_{L+1_k}w_{kj}})a_{L_j}(1-a_{L_j}) & j \in L \neq O
  \end{cases}
\end{equation}

We must calculate $\frac{\partial E_2}{\partial w_{ji}}$. This derivation is
complicated and thus we have moved this to Appendix
\todo{ref to appendix and actually write out derivation}.
We find the following gradient

\todo{insert final e2 gradient}
\begin{equation}
  \frac{\partial E_2}{\partial w_{ji}} = 
\end{equation}

In practice, we found calculating this term was quite time intensive, and thus
we cut off the gradient after one layer as follows

\begin{equation}
  \frac{\partial E_2}{\partial w_{ji}} = \sum_{p=1}^{N} -N(a_{H_{l_j}}^p - \overline{a_{H_{l_j}}})a_{H_{l_j}}^p(1-a_{H_{l_j}}^p)a_{H_{l-1_i}}
\end{equation}

This simplification means that our error function alters weight
$w_{ji}$ to maximize distances between nodes in layer $H_{l}$. In theory, this
could be problematic as weight $w_{ji}$ can affect the activations of all
hidden nodes in layers $H_{t}$ where $t \geq l$. However, in practice we
found this tweak still yielded separation of hidden nodes.

We can simply calculate $\frac{\partial E_3}{\partial w_{ji}}$ to get the following

\begin{equation}
  \frac{\partial E_3}{\partial w_{ji}} = 2 w_{ji}
\end{equation}

\subsection{Rule Extraction}
\label{sec:re}

Our rule extraction algorithm is similar to the one in the paper that inspired our work \cite{thuan11}, with changes in the later steps to account for multiple hidden layers. Given n hidden layers, we:
\begin{enumerate}
\item
1: Train the network
\item
2: Cluster each hidden layer node’s activation values 
\item
3: Extract rules for the output in terms of the nth hidden layer’s clustered activation levels using C5.0 rules
\item
4: Extract rules for all hidden layers 2 through n – 1 in terms of the next hidden layer’s rules using C5.0 rules
\item
5: Combine the rules at each layer to generate network rules
\end{enumerate}
In step 4, we had a choice to cluster the middle hidden layer nodes according to the eventual correct class, or according to the following hidden layer. We chose the first option, because of the notion that hidden layers model features, thus it was important to ensure the clustering was would help the algorithm differentiate the feature or decision being modeled at each node in each layer. Connecting this process directly to the output would’ve circumvented context not immediately available to those nodes (although signal from output does affect it because of back propagation). This choice also made connecting the rules generated at each level a bit more intuitive, as each set of rules was modeled on the next. We perform these steps on networks that have been trained with only E1 error as the control, and also on our modified network that incorporates E2. We additionally performed trials on single hidden layer networks in addition to our multi layer ones to form a baseline for comparison. 

% From Reggia: How'd you do the work?
