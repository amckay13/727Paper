\section{Methodology}
\label{sec:method}

\subsection{New Error Term}
\label{sec:nn}

In this paper, we are considering standard feed-forward networks with
sigmoid activation functions for all nodes in the network. More formally,
for all layers $l$ in the network, the activation of the $j^{th}$ node
$a_{H_{l_j}}^p$ for the $p^{th}$ input pattern is given by

\begin{equation}
  a_{H_{l_j}}^p = \sigma(\sum_{i\in H_{l-1}}{w_{ji}a_{H_{l-1_i}}^p})
\end{equation}

where $\sigma$ is the standard sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$
and $w_{ji}$ is the weight from node $i$ in the previous layer to node $j$.

We use standard mean-squared error term
\footnote{
It should be noted that using
mean-squared error is not standard for neural networks used for
classification, however we use mean-squared error as this is the error
used by \cite{thuan11} who use a similar approach for simplifying rule
extraction in single-layer feed-forward nets.
}

\begin{equation}
  E_1 = \frac{1}{2}\sum_{p=1}^{N} \sum_{k \in output} (t_k^p-a_{O_k}^p)^2
\end{equation}

where $N$ is the number of input patterns and $t_k^p$ is the target activation
of output node $k$ for input pattern $p$.

We introduce a new error term $E_2$ which is proportional to the distance between
activations in each hidden layer

\begin{equation}
  E_2 = -\frac{1}{2}\sum_{p=1}^{N} \sum_{q=1}^{N} \sum_{l=1}^{M} \sum_{k \in H_l} (a_{H_{l_k}}^p - a_{H_{l_k}}^q)^2
\end{equation}

where $M$ is the number of hidden layers.

We add a third error term $E_3$ that aims to minimize our weights, given
by

\begin{equation}
  E_3 = \sum_{j} \sum_{i} w_{ji}^2
\end{equation}

We define our total error term as

\begin{equation}
  E = \alpha E_1 + \beta E_2 + \lambda E_3
\end{equation}

where $0 \leq \beta \leq 1$ and $\alpha = 1 - \beta$.
We can view the problem as a multi-objective optimization:
minimizing $E_1$ will lead to better accuracy while
minimizing $E_2$ will create more separation between activations of
nodes in the hidden layers. The goal of the $\alpha$ and $\beta$
multipliers is to balance the error such that we achieve
both good accuracy and separation. The $E_3$ term is included
to minimize our weights; large weights will push our sigmoid
activations to $1$ which will in turn create less separation in
hidden node activations.

We calculate our learning rule based on $E$ via

\begin{equation}
  \frac{\partial E}{\partial w_{ji}} = \alpha \frac{\partial E_1}{\partial w_{ji}} + \beta \frac{\partial E_2}{\partial w_{ji}} + \lambda \frac{\partial E_3}{\partial w_{ji}}
\end{equation}

where $\frac{\partial E_1}{\partial w_{ji}}$ is the standard mean-squared error gradient

\begin{equation}
  \frac{\partial E_1}{\partial w_{ji}} = -\delta_{L_j}a_{L-1_i} 
\end{equation}

where $L$ is the layer that contains node $j$, $L-1$ is the previous layer
which contains node $i$, and $\delta_{L_j}$ is given by

\begin{equation}
  \delta_{L_j} =
  \begin{cases}
    (t_{O_j} - a_{O_j})a_{O_j}(1-a_{O-j}) & j \in L = O \\
    (\sum_{k \in L+1}{\delta_{L+1_k}w_{kj}})a_{L_j}(1-a_{L_j}) & j \in L \neq O
  \end{cases}
\end{equation}

We must calculate $\frac{\partial E_2}{\partial w_{ji}}$. This derivation is
complicated and thus we have moved this to Appendix
\todo{ref to appendix and actually write out derivation}.
We find the following gradient, for node $j$ in hidden layer $c$
\begin{equation}
    \frac{\partial E_2}{\partial w_{ji}} = \sum_{p=1}^N \frac{\partial E_2^p}{\partial w_{ji}}
\end{equation}

\begin{align}
    \frac{\partial E_2^p}{\partial w_{ji}} =
    -N(a_{H_{l_j}}^p - \overline{a_{H_{c_j}}})f'(in_{H_{c_j}})a_{H_{c-1_i}} \notag\\
    -\sum_{l>c}\sum_{k\in H_{l}}\sum_{q=1}^N (a_{H_{l_k}}^p - a_{H_{l_k}}^q)  \Bigg[f'(in_{H_{l_k}}^p)\delta_{H_{l_k}}^p - f'(in_{H_{l_k}}^q)\delta_{H_{l_k}}^q  \Bigg]
\end{align}
\begin{align}
    \delta_{H_{l_k}}^p = 
     \begin{cases}
         \sum_{H_{l-1_{g}}\in H_{l-1}} w_{H_{l_k}H_{l-1_{g}}} \frac{\partial a_{H_{l-1_{g}}}^p}{\partial w_{ji}} & l > c \\
         \begin{cases}
            a_i^p & l_k = j \\
            0 & l_k \neq j
         \end{cases} & l = c
    \end{cases} 
\end{align}

In practice, we found calculating this term was quite time intensive, and thus
we cut off the gradient after one layer as follows

\begin{equation}
  \frac{\partial E_2}{\partial w_{ji}} = \sum_{p=1}^{N} -N(a_{H_{l_j}}^p - \overline{a_{H_{l_j}}})a_{H_{l_j}}^p(1-a_{H_{l_j}}^p)a_{H_{l-1_i}}
\end{equation}

This simplification means that our error function alters weight
$w_{ji}$ to maximize distances between nodes in layer $H_{l}$. In theory, this
could be problematic as weight $w_{ji}$ can affect the activations of all
hidden nodes in layers $H_{t}$ where $t \geq l$. However, in practice we
found this tweak still yielded separation of hidden nodes.

We can simply calculate $\frac{\partial E_3}{\partial w_{ji}}$ to get the following

\begin{equation}
  \frac{\partial E_3}{\partial w_{ji}} = 2 w_{ji}
\end{equation}

\subsection{Rule Extraction}
\label{sec:re}

Our rule extraction algorithm is similar to the one in the paper that inspired our work \cite{thuan11}, with changes in the later steps to account for multiple hidden layers. Given $l$ hidden layers, we:
\begin{enumerate}
\item
Train the network
\item
For each hidden layer, beginning at the nth layer and working backwards:
\subitem
a) Cluster each hidden layer node’s activation values 
\subitem
b) At the $l^{th}$ hidden layer: Use C5.0 \cite{C5} to extract rules from
the $l^{th}$ layer in terms of the output
\subitem
c) At any other hidden layer $k$: Use C5.0 to extract rules from
the current layer $k$ in terms of the $k+1$ layer's clustered activations
\item
Combine the rules at each layer to generate network rules
\item 
Calculate rule accuracy on the validation set using the generated network rules
\end{enumerate}

In step 4, we had a choice to cluster the middle hidden layer nodes
according to the eventual correct class, or according to the
preconditions of the rules in the following hidden layer. We chose the
first option, because of the notion that hidden layers model features,
thus it was important to ensure the clustering was would help the
algorithm differentiate the feature or decision being modeled at each
node in each layer. Because all the datasets we worked with contained
continuous input, both steps 3 and 4 created a series of intervals [$i_1$,$i_2$) for
each hidden node, which constrain the activation of the $j^{th}$ node in hidden
layer $l$ as follows:
\begin{equation}
  \begin{split}
    &i_1 \le a_{H_{l_j}} \textless i_2 \\
    &i_1 \le \sigma(w_{j1}x_1 + w_{j2}x_2 + ... + w_{jn}x_n)  \textless i_2
  \end{split}
\end{equation}
Then, at every layer $l$, C5.0 creates rules that determine which intervals a node's
activation needs to fall in in order to result in 1) an output class c
for step 3:
\begin{equation}
  i_1 \le a_{H_{l_j}} \textless i_2  \longrightarrow  c
\end{equation}
or 2) the precondition of a rule in the following hidden layer that
will be satisifed, given thart any rule $r$ in layer $k + 1$ is in the form preconditions
$\longrightarrow$ postcondition:
\begin{equation}
  i_1 \le a_{H_{l_j}}  \textless i_2  \longrightarrow preconditions_r \epsilon r_{k+1}
\end{equation}

 Connecting the rules at middle hidden layers directly to the output
 would’ve circumvented context not immediately available to those
 nodes (although signal from output does affect it because of back
 propagation). This choice also made connecting the rules generated at
 each level a bit more intuitive, as each set of rules was modeled on
 the next. An example of a generated rule for the \todo{insert
   reference to example here}.

It is also important to note that while the paper
that inspired this work \cite{thuan11} used the Chi2 discretization
algorithm to cluster the activations, we chose to sort the intervals
and then merge consecutive intervals whose output class was the same. We perform these steps on networks that have been trained with only $E_1$
error as the control, and also on our modified network that
incorporates $E_2$. We additionally performed trials on single hidden
layer networks in addition to our multi layer ones to form a baseline
for comparison. 

% From Reggia: How'd you do the work?
